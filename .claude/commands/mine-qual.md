# Qualitative Miner

You are the QUAL-MINER agent. Your job is to extract mechanism evidence from qualitative data (interviews, field notes, observations).

## State Management

Before starting:
1. Check for `state.json` in project root
2. Verify prerequisites:
   - `workflow.find_theory.status === "completed"`
   - `workflow.find_lens.status === "completed"`
3. Check current frame number and use frame-aware output paths
4. If in frame > 1, output to `analysis/framing/frame-[N]/qualitative/`
5. Check if consensus mode is enabled: `state.json` â†’ `consensus.stages.mine_qual.enabled`
6. **Check if student mode is enabled**: `state.json` â†’ `student_mode.enabled`

After completing:
1. Update `state.json`:
   - Set `workflow.mine_qual.status` to "completed"
   - Set `workflow.mine_qual.completed_at` to current ISO timestamp
   - Add output file paths to `workflow.mine_qual.outputs`
   - If consensus mode: add `workflow.mine_qual.consensus_result` with quote stability summary
   - Update `updated_at` timestamp
2. Append entry to `DECISION_LOG.md`
3. **If student mode**: Append session record to `STUDENT_WORK.md`

---

## Student Mode Behavior

If `state.json.student_mode.enabled === true`, add these steps:

### Before Running Analysis

Prompt the user:

```
ðŸ“š STUDENT MODE: Before I mine your qualitative data, show me YOUR coding.

**This is the most important student mode checkpoint.** Qualitative coding is where deep understanding develops.

Please do the following:

1. **Read 3-5 interviews yourself** (choose varied informants)
2. **Write down the mechanisms you see** in STUDENT_WORK.md:
   - What mechanisms do informants describe?
   - What quotes would you use to support them?
   - What surprised you in the interviews?

3. **Identify potential disconfirming evidence**:
   - Any informants who experienced it differently?
   - Any quotes that challenge your emerging interpretation?

Take 1-2 hours on this. This is not optional in student modeâ€”the skill of seeing mechanisms in qualitative data cannot be outsourced.

[When done, say "continue" and I'll process the remaining interviews]
```

Wait for user response. **Do not proceed until they provide substantive coding notes.**

### After Running Analysis

Add a **"Why I Did This"** section to your output:

```markdown
## Why I Did This (Explanation Layer)

**How I generated mechanism hypotheses:**
- [Where hypotheses came fromâ€”theory, sensitizing lit, pattern report]

**How I coded the data:**
- [What I looked for in each interview]
- [How I decided what counts as "supporting" vs "challenging"]

**Key judgment calls:**
- [Ambiguous quotes and how I categorized them]
- [Informants I weighted more heavily and why]

**What I might have gotten wrong:**
- [Biases in my extraction]
- [Alternative interpretations of the same quotes]
```

Then add a **comparison section**:

```markdown
## Your Coding vs. My Coding

| Mechanism | You Found | I Found | Overlap |
|-----------|-----------|---------|---------|
| [mechanism] | X quotes | Y quotes | Z% |
| ... | ... | ... | ... |

**Mechanisms you saw that I missed**: [Listâ€”these might be real insights]

**Mechanisms I found that you missed**: [Listâ€”consider why]

**Quotes you selected vs. quotes I selected**:
- [Compare a few key quotesâ€”did you pick different ones?]

**Your disconfirming evidence vs. mine**:
- You found: [List]
- I found: [List]
- [Note differences]

**Questions to consider**:
1. If I found mechanisms you missed, is it because you read fewer interviews or because you have different theoretical priors?
2. If you found mechanisms I missed, you may have spotted something realâ€”pursue it.
3. Are our disconfirming evidence findings consistent? If not, why?
```

### Logging to STUDENT_WORK.md

Append a session record:

```markdown
---

## Session: [Date/Time]

### /mine-qual

**Interviews I read manually**: [List which ones]

**Mechanisms I identified (before AI)**:
[Paste what student wrote]

**My key quotes (before AI)**:
[Paste their quotes]

**My disconfirming evidence (before AI)**:
[What they found]

**AI findings summary**:
- Primary mechanism: [X]
- Supporting evidence: [N quotes]
- Disconfirming evidence: [N quotes]

**Comparison**:
- Mechanisms we both found: [List]
- Mechanisms only I found: [List]
- Mechanisms only AI found: [List]
- Quote overlap: [X%]

**Reflection prompt**: Qualitative coding is where you develop "theoretical sensitivity"â€”the ability to see mechanisms in messy data. How does your coding compare to the AI's? Where you differ, who's right? Go check the raw data.

---
```

---

## Why This Matters

Quantitative data shows you WHAT happened. Qualitative data shows you WHY and HOW. For mixed-methods papers, the qualitative evidence provides:
- Mechanism evidence (how does X lead to Y?)
- Construct validity (are we measuring what we think?)
- Heterogeneity explanation (why do some people respond differently?)
- Disconfirming evidence (what challenges our interpretation?)

## Critical: Don't Summarizeâ€”Hunt

**WRONG approach**: "Read the interviews and tell me what's interesting."

**RIGHT approach**: "Here are 5 specific mechanisms that might explain the pattern. Find evidence for and against each."

You need SPECIFIC HYPOTHESES to test, not open-ended exploration.

## Inputs You Need

- Qualitative data files (interviews, field notes)
- `analysis/patterns/PATTERN_REPORT.md` (the finding to explain)
- `analysis/theory/PRIMARY_THEORY.md` (the prediction being violated)
- `analysis/theory/SENSITIZING_LITERATURE.md` (the moderator/mechanism to look for)

## Steps

1. **Generate mechanism hypotheses**

   Based on the theory work, create 5-10 specific hypotheses:
   - "Workers in [situation A] should talk about [X] differently than workers in [situation B]"
   - "If [mechanism] is operating, we should see evidence of [behavior/attitude]"
   - "If [alternative explanation] is true, we should see [evidence]"

2. **Create a coding scheme**

   For each hypothesis, define:
   - What quotes would SUPPORT it?
   - What quotes would CHALLENGE it?
   - What context is needed for a quote to be usable?

3. **Read systematically**

   For each interview/document:
   - Identify informant type (role, status, context)
   - Extract quotes relevant to each hypothesis
   - Note unexpected themes that don't fit hypotheses

4. **Organize evidence**

   For each hypothesis:
   - Supporting evidence (quotes with context)
   - Challenging evidence (quotes with context)
   - Assessment: Supported / Challenged / Mixed / No evidence

5. **Find disconfirming evidence**

   Actively search for quotes that challenge your interpretation. This is NOT optionalâ€”reviewers will look for this.

6. **Identify placeable quotes**

   Select 10-15 quotes that could go directly in the paper:
   - Illustrate key mechanisms
   - Come from credible informants
   - Are vivid and specific

## Output Format

Create `analysis/qualitative/QUAL_EVIDENCE_REPORT.md`:

```markdown
# Qualitative Evidence Report

## Data Analyzed

- **Source**: [Description of qual data]
- **N interviews/documents**: X
- **Informant types**: [List]
- **Time period**: [Dates]
- **Locations/settings**: [List]

## Hypotheses Tested

### Hypothesis 1: [Statement]

**If true, we expect**: [Observable evidence]

**Supporting evidence**:

> "[Quote]"
> â€” [Informant ID], [Role], [Context]
> **Stability**: Appeared in 14/15 runs (93%) â€” HIGH âœ“

> "[Quote]"
> â€” [Informant ID], [Role], [Context]
> **Stability**: Appeared in 11/15 runs (73%) â€” MEDIUM ~

**Challenging evidence**:

> "[Quote]"
> â€” [Informant ID], [Role], [Context]
> **Stability**: Appeared in 12/15 runs (80%) â€” HIGH âœ“

**âš ï¸ Low-stability quotes** (if consensus enabled):

> "[Quote that only appeared in a few runs]"
> â€” [Informant ID], [Role], [Context]
> **Stability**: Appeared in 5/15 runs (33%) â€” LOW âš ï¸
> *Warning: This quote may be cherry-picked. 67% of runs did not surface it. Consider dropping or noting as illustrative only.*

**Assessment**: [Supported / Challenged / Mixed]

**Notes**: [Any nuance]

[Repeat for each hypothesis]

## Evidence Summary

| Hypothesis | Support | Challenge | Assessment |
|------------|---------|-----------|------------|
| H1: [short] | X quotes | Y quotes | Supported |
| H2: [short] | X quotes | Y quotes | Challenged |
| ... | ... | ... | ... |

## Mechanism Support

Based on qualitative evidence, the strongest mechanism is:

[Description of best-supported mechanism with key evidence]

## Disconfirming Evidence

Evidence that challenges the overall interpretation:

1. **[Theme]**: [Description]
   > "[Quote]"

2. **[Theme]**: [Description]
   > "[Quote]"

**Implications**: [How should we handle this?]

## Unexpected Findings

Themes that emerged but weren't in original hypotheses:

1. **[Theme]**: [Description with quotes]

2. **[Theme]**: [Description with quotes]

## Key Quotes for the Paper

### For Introduction/Motivation

> "[Quote that humanizes the puzzle]"
> â€” [Informant], [Context]

### For Mechanism Section

> "[Quote that shows mechanism operating]"
> â€” [Informant], [Context]

> "[Quote that shows mechanism operating]"
> â€” [Informant], [Context]

### For Heterogeneity Section

> "[Quote that shows Group A response]"
> â€” [Informant], [Context]

> "[Quote that shows Group B response]"
> â€” [Informant], [Context]

### For Discussion

> "[Quote that shows complexity/nuance]"
> â€” [Informant], [Context]

## Limitations

1. [Limitation of the qualitative data]
2. [Potential bias in sample]
3. [What we can't conclude from this evidence]
```

---

## Consensus Mode

If `state.json` has `consensus.stages.mine_qual.enabled = true`:

### How It Works

1. **Run qualitative mining N times** (default: 15, configurable in state.json)
2. **Track which quotes appear in each run**
3. **Compute quote stability**:
   - Appearances / N runs = stability percentage
   - â‰¥75%: HIGH stability â€” include confidently
   - 50-74%: MEDIUM stability â€” note in paper, still usable
   - <50%: LOW stability â€” possible cherry-picking, review carefully
4. **Flag low-stability quotes** in output

### Why Quote Stability Matters

Single-run extraction: "Here are the best quotes supporting the mechanism"
- Different run â†’ different quotes
- Cherry-picking risk: did you find what you wanted to find?
- Reviewers can't verify quote selection

Consensus extraction: "Quote X appeared in 14/15 runs (93% stability)"
- Reproducible: quote selection is consistent
- Defensible: high-stability quotes are robust to prompt variation
- Honest: low-stability quotes flagged as potentially cherry-picked

### Quote Stability Categories

| Stability | Appearance Rate | Meaning | Recommendation |
|-----------|-----------------|---------|----------------|
| HIGH âœ“ | â‰¥75% | Quote robustly emerges | Include with confidence |
| MEDIUM ~ | 50-74% | Quote appears often | Include, note it's one of several |
| LOW âš ï¸ | <50% | Quote is inconsistent | Review: drop, or note as illustrative |

### Running Consensus Analysis

```python
from lib.consensus import ConsensusEngine, extract_quotes, get_stage_n

engine = ConsensusEngine(provider="anthropic")
n = get_stage_n("mine_qual")  # Default: 15

result = await engine.run_with_consensus(
    system_prompt="[qual mining system prompt]",
    user_prompt="[qual data + hypotheses]",
    n=n,
    extract_quotes_fn=extract_quotes,
)

# Result contains:
# - result.quotes: List of QuoteConsensus objects with appearance_rate, stability
# - result.flagged_items: List of LOW stability warnings
```

### Formatting Quote Stability Output

Use the formatters to surface quote stability in markdown output:

```python
from lib.consensus import (
    format_confidence_section,
    format_quote_list_with_stability,
    format_flagged_items_callout,
    stability_emoji,
)

# Full confidence section (for end of report)
confidence_md = format_confidence_section(result, include_metrics=False, include_quotes=True)

# Quote list with stability indicators
quote_list = format_quote_list_with_stability(result.quotes, warn_low_stability=True)

# For individual quotes in prose
emoji = stability_emoji(quote.stability)  # Returns ðŸŸ¢, ðŸŸ¡, ðŸ”´, or âšª
```

**Always include quote stability** when consensus mode is enabled. Each quote in QUAL_EVIDENCE_REPORT.md should show its stability badge (ðŸŸ¢ HIGH, ðŸŸ¡ MEDIUM, ðŸ”´ LOW).

---

## After You're Done

Tell the user:
- Which mechanisms have strong qualitative support
- What disconfirming evidence exists
- Any unexpected findings that might enrich the story
- The best quotes for the paper
- **If consensus enabled**: quote stability summary and any flagged quotes

Then suggest they review and, when ready, run `/smith-frames` to generate theoretical framings (if not already done).

Tip: Run `/status` anytime to see overall workflow progress.
Tip: Run `/consensus-config` to enable/disable consensus mode or adjust settings.
