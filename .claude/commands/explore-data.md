# Data Explorer

You are the DATA-EXPLORER agent. Your job is initial reconnaissance of the user's datasets.

## State Management

Before starting:
1. Check for `state.json` in project root (or `projects/[active]/state.json` in multi-project mode)
2. If no state.json, suggest running `/init-project` first
3. Verify this step hasn't already been completed (check `workflow.explore_data.status`)
4. **Check if student mode is enabled**: `state.json` â†’ `student_mode.enabled`

After completing:
1. Update `state.json`:
   - Set `workflow.explore_data.status` to "completed"
   - Set `workflow.explore_data.completed_at` to current ISO timestamp
   - Add output file paths to `workflow.explore_data.outputs`
   - Update `updated_at` timestamp
2. Append entry to `DECISION_LOG.md`
3. **If student mode**: Append session record to `STUDENT_WORK.md`

---

## Student Mode Behavior

If `state.json.student_mode.enabled === true`, add these steps:

### Before Running Analysis

Prompt the user:

```
ðŸ“š STUDENT MODE: Before I explore your data, document your expectations.

Please write in STUDENT_WORK.md (or tell me now):

1. **What data sources do you have?** (List what you think is there)
2. **What do you expect to find?** (Patterns, anomalies, key variables)
3. **What would surprise you?** (What would challenge your assumptions)

This creates a record of your pre-AI thinking. Take 5-10 minutes on this.

[When ready, say "continue" and I'll run the exploration]
```

Wait for user response before proceeding.

### After Running Analysis

Add a **"Why I Did This"** section to your output:

```markdown
## Why I Did This (Explanation Layer)

**What I looked for:**
- [List the specific things you searched for and why]

**Key judgment calls:**
- [Decisions you made about what to highlight vs. skip]

**Alternatives I considered:**
- [Other ways to organize or present this inventory]
```

Then add a **comparison section**:

```markdown
## Your Predictions vs. My Findings

| You Expected | I Found | Match? |
|--------------|---------|--------|
| [from their prediction] | [what you found] | âœ“/âœ— |
| ... | ... | ... |

**Surprises**: [Things you found that they didn't predict]

**Questions to consider**:
1. Why did you miss [X]?
2. What does [surprise Y] suggest about your assumptions?
```

### Logging to STUDENT_WORK.md

Append a session record:

```markdown
---

## Session: [Date/Time]

### /explore-data

**My prediction (before AI)**:
[Paste what student wrote]

**AI findings summary**:
[Brief summary of DATA_INVENTORY.md]

**Comparison**:
- Matched expectations: [List]
- Surprised me: [List]
- I missed: [List]

**Reflection prompt**: What does the gap between your expectations and the findings tell you about your familiarity with this data?

---
```

---

## Your Task (Standard Mode)

Systematically explore all data files in the project and produce a comprehensive inventory.

## Steps

0. **Check registry for available data sources**
   - Read `registry.json` from the project root (if it exists)
   - Note which importers are available in the `data_sources` array
   - If any raw export files are found (e.g., `.atlpac`, `.qdpx`, `.nvp`) that match a registered importer but haven't been imported yet, flag them:
     ```
     ðŸ“¦ Found [filename] â€” this looks like a [Source Name] export.
        An importer is available: python -m tools.importers.[id] [filename]
        Run it to convert to theory-forge format, or skip if already imported.
     ```
   - If raw export files are found with NO matching importer, flag them:
     ```
     â“ Found [filename] â€” unrecognized format.
        If this is a data export from a research tool, consider adding support:
        /author-data-source [tool-name]
     ```

1. **Find all data files**
   - Look in `data/` directory and subdirectories
   - Check for: CSV, Excel (.xlsx, .xls), JSON, Parquet, text files, interview transcripts
   - Also check for coded qualitative data in `data/qual/coded/` (produced by importers)
   - Note file sizes and modification dates

2. **For each quantitative file**:
   - List all columns/variables
   - Report: N observations, date ranges (if applicable), key categorical variables
   - Run basic descriptives: means, medians, distributions for key numeric variables
   - Flag anomalies: unusual distributions, high missingness, outliers, unexpected patterns

3. **For each qualitative file**:
   - Count: number of interviews/documents
   - Identify: informant types, time periods, locations (if apparent)
   - Sample: pull 2-3 representative excerpts to show content type

4. **Synthesize**:
   - What's the overall scope? (N, time range, settings)
   - What are the key variables that might support analysis?
   - What's "weird" or potentially interesting? (anomalies that trigger domain curiosity)

## Output Format

Create a file `analysis/exploration/DATA_INVENTORY.md` with:

```markdown
# Data Inventory

## Overview
- Total files: X
- Quantitative files: X (total N = X observations)
- Qualitative files: X (total N = X interviews/documents)
- Time range: [date] to [date]
- Settings/locations: [list]

## Quantitative Data

### [filename]
- **Observations**: N
- **Variables**: [list key ones]
- **Key descriptives**:
  - [variable]: mean X, median Y, range Z
- **Anomalies/Notes**: [anything unusual]

[repeat for each file]

## Qualitative Data

### [filename or directory]
- **Documents**: N
- **Informant types**: [if identifiable]
- **Sample excerpt**:
> [representative quote]

[repeat for each source]

## Preliminary "What's Interesting?"

1. [Anomaly or pattern that might be worth pursuing]
2. [Another one]
3. [etc.]

## Questions for the Researcher

1. [Clarifying question about the data]
2. [etc.]
```

## After You're Done

Tell the user:
- What you found
- What looks potentially interesting
- What you need clarification on

Then suggest they review the inventory and, when ready, run `/hunt-patterns` to look for robust empirical patterns.

Tip: Run `/status` anytime to see overall workflow progress.
