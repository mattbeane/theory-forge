# Theory-Forge Backlog

## Priority 1: Immediate / High Impact

### 1.1 Skill-Forge Gate for New Users
**Status:** ‚úÖ Complete
**Why:** PhD students may use theory-forge as a crutch before developing judgment. The tool should detect this and redirect.

**Implementation:**
- ‚úÖ Added to `/init-project` - Step 0: Experience Check
- ‚úÖ Prompt: "Have you independently written and submitted (or published) a mixed-methods paper?"
- ‚úÖ If no: Warning message with skill-forge redirect
- ‚úÖ Three options: proceed anyway (logged), go to skill-forge, cancel
- ‚úÖ Decisions logged to DECISION_LOG.md

---

### 1.2 Guided First Project Experience
**Status:** ‚úÖ Complete
**Why:** Current onboarding assumes expertise. New users see wall of commands.

**Implementation:**
- ‚úÖ Added `--guided` flag to `/init-project`
- ‚úÖ 6-step walkthrough with explanations
- ‚úÖ Progress indicators: "STEP X OF 6"
- ‚úÖ "Why this matters" for each component
- ‚úÖ Full pipeline overview at completion

---

### 1.3 Failure Recovery with Specific Fixes
**Status:** ‚úÖ Complete
**Why:** Gate failures feel like dead ends.

**Implementation:**
- ‚úÖ Created `gate-failure-templates.md` with standardized messages for all gates
- ‚úÖ Each failure includes: what happened, why it matters, 3-4 options for next steps
- ‚úÖ Specific guidance per gate (A through F)
- ‚úÖ "Common mistake" warnings
- ‚úÖ Retry instructions

---

### 1.4 Modular Use (Run Single Evals)
**Status:** ‚úÖ Complete
**Why:** Researchers with existing papers should be able to run just `/eval-genre` without full pipeline.

**Implementation:**
- ‚úÖ Created `/eval` command for running any evaluation independently
- ‚úÖ Usage: `/eval genre`, `/eval zuckerman --file paper.md`, `/eval becker --quick`
- ‚úÖ `--quick` flag for verdict-only output
- ‚úÖ `--fix` flag to generate suggested fixes
- ‚úÖ Documents all available evaluations and when to use each

---

## Priority 2: Important / Medium Term

### 2.1 Example Worked Project
**Status:** Not started
**Why:** Users need to see what "done" looks like at each stage.

**Implementation:**
- Use surgery data ‚Üí shadow learning paper as canonical example
- Create `/examples/shadow-learning/` with all artifacts
- Each pipeline stage has "here's what this looked like for shadow learning"
- Link from documentation

---

### 2.2 Messaging: AI-Assisted, Human-Directed
**Status:** ‚úÖ Complete (partial - README updated)
**Why:** Reduce anxiety, clarify value prop.

**Implementation:**
- ‚úÖ README: "This Tool Is For Experienced Researchers" section
- ‚úÖ Clear "What it does" vs "What it doesn't do" lists
- ‚úÖ "Researchers who can supervise AI are more valuable than ever"
- ‚úÖ "Your job is to catch when the AI is wrong. That skill is why you're irreplaceable."
- ‚è≥ Pipeline output highlighting (future)
- ‚è≥ Credit lines on outputs (future)

---

### 2.3 Confidence Indicators
**Status:** ‚úÖ Complete
**Why:** Users trust AI output too much or too little.

**Implementation:**
- ‚úÖ Created `lib/consensus/formatters.py` with markdown formatting functions
- ‚úÖ Stability badges: üü¢ HIGH, üü° MEDIUM, üî¥ LOW, ‚ö™ UNKNOWN
- ‚úÖ Full confidence section formatter for command output
- ‚úÖ Quote stability display for qualitative mining
- ‚úÖ Flagged items callout for items needing review
- ‚úÖ Updated `/hunt-patterns` to use formatters
- ‚úÖ Updated `/mine-qual` to use formatters
- ‚úÖ Created `docs/CONFIDENCE_INDICATORS.md` documentation

---

### 2.4 Import/Export Integration
**Status:** Atlas.ti parser exists; others don't
**Why:** Researchers have existing tools.

**Implementation:**
- NVivo import
- Overleaf export (direct push)
- Word export
- Zotero integration for references

---

### 2.5 Audit Trail / Transparency
**Status:** state.json exists but incomplete
**Why:** No black boxes.

**Implementation:**
- Log every AI decision with reasoning
- Reviewable trail: "At 2:30pm, AI identified pattern X because..."
- Export audit trail for methods appendix

---

## Priority 3: Nice to Have / Long Term

### 3.1 Video Walkthroughs
**Status:** Not started
**Why:** Some users learn better watching.

**Implementation:**
- Record guided project walkthrough
- Stage-by-stage explanations
- "What to do when Gate D fails" video
- Host on YouTube, link from docs

---

### 3.2 Common Mistakes Documentation
**Status:** Not started
**Why:** Proactive failure prevention.

**Implementation:**
- For each gate: top 3 failure reasons
- Before/after examples
- "Red flag" patterns to avoid
- Add to `/docs/common-mistakes/`

---

### 3.3 Web Interface
**Status:** Not started
**Why:** Not everyone is comfortable with CLI.

**Implementation:**
- Web app wrapping core functionality
- Visual pipeline progress
- Point-and-click eval running
- Consider: hosted vs. self-hosted

---

### 3.4 Collaborative Mode
**Status:** Not started
**Why:** Research is often collaborative.

**Implementation:**
- Multi-user projects
- Track who made which decisions
- Comment threads on artifacts
- Version control integration

---

## Technical Debt

### T1: Consistent Output Formats
All commands should produce consistent markdown/JSON.

### T2: Error Handling
Graceful failures with helpful messages.

### T3: Testing
Automated tests for pipeline stages.

### T4: Documentation Audit
Ensure all commands are documented with examples.

---

## Completed

- [x] P1.1: Skill-forge gate for new users (2025-02-04)
- [x] P1.2: Guided first project experience (2025-02-04)
- [x] P1.3: Failure recovery with specific fixes (2025-02-04)
- [x] P1.4: Modular eval use (2025-02-04)
- [x] P2.2: AI-assisted messaging (README portion) (2025-02-04)
- [x] Post-revision genre re-check warning (2025-02-04)
- [x] Level 4 Capstone in skill-forge (linked tool) (2025-02-04)
- [x] Ecosystem framing with skill-forge (2025-02-04)
- [x] P2.3: Confidence indicators in output (2025-02-04)

---

*Last updated: 2025-02-04*
