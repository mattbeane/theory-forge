# Paper Mining Agents: PhD Student Session

**For TM PhD Students — January 21, 2025**

---

## Part 1: Shock and Awe Demo (~20 min)

### The "Wow"
1. Live demo: Feed in a paper they know, watch extraction in real-time
2. Show the full pipeline: `/explore-data` → `/hunt-patterns` → `/mine-qual`
3. Aggregate patterns across 10-20 papers in minutes
4. Output that would take days by hand

### The Pivot
Then show where it fails:
- A hallucinated quote
- A misclassified mechanism
- A confident answer that missed the obvious

**Setup line**: "This is a power tool. Power tools amplify whatever you point them at—including your mistakes."

---

## Part 2: Debrief Discussion (~40 min)

### Critique
- Where does this break down? (They just saw failure modes)
- What can't it do? (Theoretical taste, knowing what's interesting, field knowledge)
- Nguyen & Welch's concerns—are they right? When?
- What would happen if you used this mindlessly?

### Extrapolation
- What else could this approach do? (Literature reviews, proposal writing, data exploration)
- What would a "good" versus "bad" use of this look like?
- How would you know if the output was wrong?

### Strategies for Learning and Career
- The skill isn't using the tool—it's supervising it
- Calibration comes from doing things the hard way first
- Taste can't be automated; developing taste is the job

---

## Part 3: The Practical Move

### "Got any data lying around—or could we get more?"

The immediate opportunity: go ask faculty about dormant data and dormant access.

- Professors accumulate data they never published
- They also have relationships with field sites they haven't used in years
- Old qualifying exam data, failed projects, side explorations
- Access that could yield complementary data for another paper

**The pitch to faculty**: "Do you have data that never became a paper? Or could we go back to your field sites and collect something new to complement what you've got?"

This is:
- Low-risk for them (data exists, or access already established)
- High-learning for you (real data, real stakes, possibly real fieldwork)
- Relationship-building (you're offering value, not asking for it)
- Practice supervising the tool on something that matters

### What to do next
1. Identify 2-3 faculty whose work interests you
2. Ask: "Do you have datasets that never became papers—or field site relationships we could use to collect more?"
3. Propose a collaboration: their data/access + their guidance + your time with these tools
4. Learn by doing—on real stakes, with real supervision

---

## Resources

- This repo: `github.com/[mattbeane]/paper-mining-agents`
- Nguyen & Welch (2025) critique (PDF provided)
- Zuckerman "Tips for Writers" (in repo)
- `nguyen-welch-comparison.md` — how this workflow addresses the critique

---

## The One Thing to Remember

The tool finds and retrieves. You interpret and judge.

If you catch yourself accepting output without scrutiny, stop.
